# src/utils/unsw_nb15_loader.py
"""
UNSW-NB15 æ•°æ®é›†åŠ è½½å™¨ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰
ä¿®å¤é—®é¢˜ï¼š
1. ç§»é™¤æœªå®ç°çš„ UNSWNB15Dataset ç±»ä¾èµ–
2. ä¿®å¤ Non-IID æ•°æ®åˆ†å‰²çš„ NumPy æ•°ç»„è½¬æ¢
3. ç»Ÿä¸€æ•°æ®æ¥å£ï¼ˆç›´æ¥ä½¿ç”¨ NumPy æ•°ç»„ï¼‰
4. æ·»åŠ å®Œæ•´çš„é”™è¯¯æ£€æŸ¥å’Œç±»å‹éªŒè¯
"""
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import DataLoader, TensorDataset
import torch
from typing import Tuple, List


class FederatedUNSWNB15:
    """
    è”é‚¦å­¦ä¹ åœºæ™¯çš„ UNSW-NB15 æ•°æ®åŠ è½½å™¨ï¼ˆå®Œæ•´é‡æ„ç‰ˆï¼‰

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - åŠ è½½å¹¶é¢„å¤„ç† UNSW-NB15 æ•°æ®é›†
    - ä½¿ç”¨ Dirichlet åˆ†å¸ƒåˆ›å»º Non-IID å®¢æˆ·ç«¯åˆ†å‰²
    - æä¾›è®­ç»ƒ/æµ‹è¯• DataLoader
    - æ”¯æŒæ”»å‡»ç‡æŸ¥è¯¢

    ä½¿ç”¨åœºæ™¯ï¼š
    - 10 ä¸ªå®¢æˆ·ç«¯ä»£è¡¨ä¸åŒç½‘ç»œåŒºåŸŸ
    - é«˜åº¦å¼‚æ„çš„æ”»å‡»æ¨¡å¼ï¼ˆNon-IIDï¼‰
    - ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒ
    """

    def __init__(
            self,
            train_path: str,
            test_path: str,
            n_clients: int = 10,
            alpha: float = 0.5,
            seed: int = 42
    ):
        """
        åˆå§‹åŒ–è”é‚¦æ•°æ®åŠ è½½å™¨

        Args:
            train_path: è®­ç»ƒé›† CSV è·¯å¾„
            test_path: æµ‹è¯•é›† CSV è·¯å¾„
            n_clients: å®¢æˆ·ç«¯æ•°é‡
            alpha: Dirichlet åˆ†å¸ƒå‚æ•°ï¼ˆè¶Šå°è¶Šä¸å¹³è¡¡ï¼‰
            seed: éšæœºç§å­
        """
        print(f"\nğŸ”„ Initializing FederatedUNSWNB15...")
        print(f"   Clients: {n_clients}")
        print(f"   Alpha: {alpha}")
        print(f"   Seed: {seed}")

        self.n_clients = n_clients
        self.alpha = alpha
        self.seed = seed

        np.random.seed(seed)
        torch.manual_seed(seed)

        # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        print("\nğŸ”„ Loading and preprocessing utils...")
        self.train_data, self.test_data, self.scaler = self._load_and_preprocess(
            train_path, test_path
        )

        # åˆ†å‰²æ•°æ®åˆ°å„å®¢æˆ·ç«¯
        print("\nğŸ”„ Splitting utils into Non-IID clients...")
        self.client_data = self._split_data_noniid(self.train_data)

        # âœ… éªŒè¯åˆå§‹åŒ–æˆåŠŸ
        assert hasattr(self, 'client_data'), "client_data not created"
        assert isinstance(self.client_data, list), "client_data should be a list"
        assert len(self.client_data) == n_clients, f"Expected {n_clients} clients, got {len(self.client_data)}"

        # éªŒè¯æ¯ä¸ªå®¢æˆ·ç«¯æ•°æ®æ˜¯ NumPy æ•°ç»„
        for i, cd in enumerate(self.client_data):
            assert isinstance(cd, np.ndarray), f"Client {i} utils is {type(cd)}, expected ndarray"
            assert cd.ndim == 2, f"Client {i} utils should be 2D, got shape {cd.shape}"

        # ç¼“å­˜æ”»å‡»ç‡
        self._attack_ratios = None

        # æ‰“å°æ•°æ®åˆ†å¸ƒ
        self._print_distribution()

        print("\nâœ… FederatedUNSWNB15 initialized successfully!")

    def _load_and_preprocess(
            self,
            train_path: str,
            test_path: str
    ) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:
        """
        åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®

        è¿”å›:
            (train_data, test_data, scaler)
            - train_data: (n_samples, n_features+1) æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾
            - test_data: (n_samples, n_features+1)
            - scaler: æ‹Ÿåˆå¥½çš„ StandardScaler
        """

        # è¯»å– CSV
        print("   Loading CSV files...")
        train_df = pd.read_csv(train_path, low_memory=False)
        test_df = pd.read_csv(test_path, low_memory=False)

        # åˆ é™¤æ— ç”¨åˆ—
        cols_to_drop = ['id', 'attack_cat']
        train_df = train_df.drop(cols_to_drop, axis=1, errors='ignore')
        test_df = test_df.drop(cols_to_drop, axis=1, errors='ignore')

        # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆç¼–ç ä¸ºæ•°å€¼ï¼‰
        categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
        if 'label' in categorical_cols:
            categorical_cols.remove('label')

        print(f"   Encoding {len(categorical_cols)} categorical features...")
        for col in categorical_cols:
            le = LabelEncoder()
            # åˆå¹¶è®­ç»ƒ+æµ‹è¯•é›†çš„ç±»åˆ«ï¼ˆç¡®ä¿æµ‹è¯•é›†ç¼–ç ä¸€è‡´ï¼‰
            all_values = pd.concat([train_df[col], test_df[col]]).astype(str)
            le.fit(all_values)

            train_df[col] = le.transform(train_df[col].astype(str))
            test_df[col] = le.transform(test_df[col].astype(str))

        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        X_train = train_df.drop('label', axis=1).values
        y_train = train_df['label'].values

        X_test = test_df.drop('label', axis=1).values
        y_test = test_df['label'].values

        # è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼ˆ0=æ­£å¸¸, 1=æ”»å‡»ï¼‰
        y_train = (y_train != 0).astype(int)
        y_test = (y_test != 0).astype(int)

        print("\nTrain dataset:")
        print(f"  Samples: {len(X_train)}")
        print(f"  Features: {X_train.shape[1]}")
        print(f"  Classes: 2 (Binary)")
        print(f"  Attack ratio: {y_train.mean():.2%}")

        print("Test dataset:")
        print(f"  Samples: {len(X_test)}")
        print(f"  Features: {X_test.shape[1]}")
        print(f"  Attack ratio: {y_test.mean():.2%}")

        # æ ‡å‡†åŒ–
        print("\n   Standardizing features...")
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
        train_data = np.hstack([X_train, y_train.reshape(-1, 1)])
        test_data = np.hstack([X_test, y_test.reshape(-1, 1)])

        return train_data, test_data, scaler

    def _split_data_noniid(self, data: np.ndarray) -> List[np.ndarray]:
        """
        ä½¿ç”¨ Dirichlet åˆ†å¸ƒåˆ›å»º Non-IID æ•°æ®åˆ†å‰²

        Args:
            data: è®­ç»ƒæ•°æ® (n_samples, n_features+1)

        Returns:
            å®¢æˆ·ç«¯æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ NumPy æ•°ç»„
        """
        labels = data[:, -1].astype(int)
        n_classes = len(np.unique(labels))

        print(f"   Classes: {n_classes}")
        print(f"   Using Dirichlet(Î±={self.alpha}) for Non-IID split...")

        # æ”¶é›†æ¯ä¸ªå®¢æˆ·ç«¯çš„æ ·æœ¬ç´¢å¼•
        client_indices = [[] for _ in range(self.n_clients)]

        for k in range(n_classes):
            # è·å–å½“å‰ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
            idx_k = np.where(labels == k)[0]
            np.random.shuffle(idx_k)

            print(f"   Class {k}: {len(idx_k)} samples")

            # Dirichlet åˆ†å¸ƒç”Ÿæˆæ¯ä¸ªå®¢æˆ·ç«¯çš„æ ·æœ¬æ¯”ä¾‹
            proportions = np.random.dirichlet(np.repeat(self.alpha, self.n_clients))

            # æ ¹æ®æ¯”ä¾‹åˆ†é…æ ·æœ¬
            proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
            idx_k_split = np.split(idx_k, proportions)

            # å°†ç´¢å¼•æ·»åŠ åˆ°å¯¹åº”å®¢æˆ·ç«¯
            for i, idx in enumerate(idx_k_split):
                if i < self.n_clients:  # é˜²æ­¢è¶Šç•Œ
                    client_indices[i].extend(idx.tolist())

        # âœ… è½¬æ¢ä¸º NumPy æ•°ç»„
        print("\n   Converting to NumPy arrays...")
        client_data = []
        for i, indices in enumerate(client_indices):
            if len(indices) == 0:
                print(f"   âš ï¸  Warning: Client {i} has no utils!")
                # åˆ›å»ºç©ºæ•°ç»„ï¼ˆä¿æŒç»´åº¦ä¸€è‡´ï¼‰
                client_data.append(np.empty((0, data.shape[1])))
            else:
                # ä½¿ç”¨ç´¢å¼•æå–æ•°æ®
                client_data.append(data[np.array(indices)])

        # âœ… éªŒè¯è½¬æ¢æˆåŠŸ
        print(f"\nâœ… Created {len(client_data)} client datasets:")
        for i, cd in enumerate(client_data):
            assert isinstance(cd, np.ndarray), f"Client {i} utils is {type(cd)}, expected ndarray"
            print(f"   Client {i}: {len(cd):6d} samples, shape={cd.shape}")

        return client_data

    def _print_distribution(self):
        """æ‰“å°æ•°æ®åˆ†å¸ƒ"""
        print("\n" + "=" * 70)
        print("Federated Data Distribution (Non-IID)")
        print("=" * 70)

        for i, client_data in enumerate(self.client_data):
            n_samples = len(client_data)
            if n_samples == 0:
                print(f"Client {i}:     0 samples, Attack ratio: N/A")
                continue

            n_attacks = (client_data[:, -1] == 1).sum()
            attack_ratio = n_attacks / n_samples * 100
            print(f"Client {i}: {n_samples:6d} samples, Attack ratio: {attack_ratio:5.2f}%")

    def get_client_loader(
            self,
            client_id: int,
            batch_size: int = 64,
            shuffle: bool = True
    ) -> DataLoader:
        """
        è·å–æŒ‡å®šå®¢æˆ·ç«¯çš„ DataLoader

        Args:
            client_id: å®¢æˆ·ç«¯ ID (0 åˆ° n_clients-1)
            batch_size: æ‰¹é‡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®

        Returns:
            PyTorch DataLoader
        """
        if client_id < 0 or client_id >= self.n_clients:
            raise ValueError(f"client_id must be in [0, {self.n_clients - 1}], got {client_id}")

        client_data = self.client_data[client_id]

        # âœ… éªŒè¯æ•°æ®ç±»å‹
        assert isinstance(client_data, np.ndarray), f"Client {client_id} utils is {type(client_data)}"

        if len(client_data) == 0:
            raise ValueError(f"Client {client_id} has no utils")

        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        X = torch.FloatTensor(client_data[:, :-1])
        y = torch.LongTensor(client_data[:, -1].astype(int))

        dataset = TensorDataset(X, y)
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

    def get_test_loader(self, batch_size: int = 128) -> DataLoader:
        """
        è·å–æµ‹è¯•é›† DataLoader

        Args:
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            PyTorch DataLoader
        """
        X = torch.FloatTensor(self.test_data[:, :-1])
        y = torch.LongTensor(self.test_data[:, -1].astype(int))

        dataset = TensorDataset(X, y)
        return DataLoader(dataset, batch_size=batch_size, shuffle=False)

    def get_client_attack_ratio(self, client_id: int) -> float:
        """
        è·å–æŒ‡å®šå®¢æˆ·ç«¯çš„æ”»å‡»æ ·æœ¬æ¯”ä¾‹

        Args:
            client_id: å®¢æˆ·ç«¯ ID (0-based)

        Returns:
            æ”»å‡»æ ·æœ¬å æ¯” (0.0-1.0)

        Raises:
            ValueError: å¦‚æœ client_id è¶…å‡ºèŒƒå›´
            TypeError: å¦‚æœæ•°æ®ç±»å‹é”™è¯¯
        """
        # é˜²å¾¡æ€§æ£€æŸ¥
        if not hasattr(self, 'client_data'):
            raise AttributeError("client_data not initialized. Call __init__() first.")

        if client_id < 0 or client_id >= self.n_clients:
            raise ValueError(f"client_id must be in [0, {self.n_clients - 1}], got {client_id}")

        client_data = self.client_data[client_id]

        # âœ… éªŒè¯æ•°æ®ç±»å‹
        if not isinstance(client_data, np.ndarray):
            raise TypeError(f"Client {client_id} utils is {type(client_data)}, expected ndarray")

        if len(client_data) == 0:
            return 0.0

        # âœ… ç°åœ¨å¯ä»¥å®‰å…¨ä½¿ç”¨ NumPy ç´¢å¼•
        attack_count = (client_data[:, -1] == 1).sum()
        return float(attack_count / len(client_data))

    @property
    def input_dim(self) -> int:
        """è·å–è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆä¸å«æ ‡ç­¾ï¼‰"""
        return self.train_data.shape[1] - 1

    def get_all_attack_ratios(self) -> List[float]:
        """
        è·å–æ‰€æœ‰å®¢æˆ·ç«¯çš„æ”»å‡»ç‡ï¼ˆå¸¦ç¼“å­˜ï¼‰

        Returns:
            æ”»å‡»ç‡åˆ—è¡¨ [client_0_ratio, client_1_ratio, ...]
        """
        if self._attack_ratios is None:
            print("\nğŸ”„ Computing attack ratios for all clients...")
            self._attack_ratios = [
                self.get_client_attack_ratio(i)
                for i in range(self.n_clients)
            ]
            print("âœ… Attack ratios cached")

        return self._attack_ratios


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    from pathlib import Path
    import os

    print("=" * 70)
    print("UNSW-NB15 Federated Data Loader - Test")
    print("=" * 70)

    # æ‰“å°å½“å‰å·¥ä½œç›®å½•
    print(f"\nCurrent Working Directory: {os.getcwd()}")

    # è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾å½“å‰æ–‡ä»¶åœ¨ src/utils/unsw_nb15_loader.pyï¼‰
    ROOT = Path(__file__).resolve().parents[2]
    DATA_DIR = ROOT / "utils" / "unsw"

    train_path = DATA_DIR / "UNSW_NB15_training-set.csv"
    test_path = DATA_DIR / "UNSW_NB15_testing-set.csv"

    print(f"\nData paths:")
    print(f"  Train: {train_path}")
    print(f"  Test: {test_path}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not train_path.exists():
        print(f"\nâŒ Error: Training file not found at {train_path}")
        print("Please download UNSW-NB15 dataset and place it in utils/raw/")
        exit(1)

    if not test_path.exists():
        print(f"\nâŒ Error: Test file not found at {test_path}")
        exit(1)

    print("\nâœ… Data files found")

    # ========================================
    # æµ‹è¯• 1: åˆå§‹åŒ–
    # ========================================
    print("\n" + "=" * 70)
    print("Test 1: Initialization")
    print("=" * 70)

    fed_data = FederatedUNSWNB15(
        train_path=str(train_path),
        test_path=str(test_path),
        n_clients=10,
        alpha=0.5,
        seed=42
    )

    # ========================================
    # æµ‹è¯• 2: å®¢æˆ·ç«¯ DataLoader
    # ========================================
    print("\n" + "=" * 70)
    print("Test 2: Client DataLoader")
    print("=" * 70)

    client_id = 0
    loader = fed_data.get_client_loader(client_id, batch_size=32, shuffle=True)

    print(f"\nClient {client_id} DataLoader:")
    print(f"  Total samples: {len(fed_data.client_data[client_id])}")
    print(f"  Batch size: 32")
    print(f"  Batches: {len(loader)}")

    # æµ‹è¯•ç¬¬ä¸€ä¸ª batch
    for batch_x, batch_y in loader:
        print(f"\nâœ… Batch loaded successfully:")
        print(f"   Features shape: {batch_x.shape}")
        print(f"   Labels shape: {batch_y.shape}")
        print(f"   Labels: {batch_y.unique().tolist()}")
        break

    # ========================================
    # æµ‹è¯• 3: æµ‹è¯•é›† DataLoader
    # ========================================
    print("\n" + "=" * 70)
    print("Test 3: Test DataLoader")
    print("=" * 70)

    test_loader = fed_data.get_test_loader(batch_size=64)

    print(f"\nTest DataLoader:")
    print(f"  Total samples: {len(fed_data.test_data)}")
    print(f"  Batch size: 64")
    print(f"  Batches: {len(test_loader)}")

    # æµ‹è¯•ç¬¬ä¸€ä¸ª batch
    for batch_x, batch_y in test_loader:
        print(f"\nâœ… Test batch loaded successfully:")
        print(f"   Features shape: {batch_x.shape}")
        print(f"   Labels shape: {batch_y.shape}")
        break

    # ========================================
    # æµ‹è¯• 4: æ”»å‡»ç‡æŸ¥è¯¢
    # ========================================
    print("\n" + "=" * 70)
    print("Test 4: Attack Ratio Query")
    print("=" * 70)

    print("\nAttack ratios for all clients:")
    for i in range(fed_data.n_clients):
        ratio = fed_data.get_client_attack_ratio(i)
        print(f"  Client {i}: {ratio:.2%}")

    # ========================================
    # æµ‹è¯• 5: ç¼“å­˜æ”»å‡»ç‡
    # ========================================
    print("\n" + "=" * 70)
    print("Test 5: Cached Attack Ratios")
    print("=" * 70)

    ratios = fed_data.get_all_attack_ratios()
    print(f"\nâœ… All attack ratios (cached): {[f'{r:.2%}' for r in ratios]}")

    # ========================================
    # æ€»ç»“
    # ========================================
    print("\n" + "=" * 70)
    print("All Tests PASSED! âœ…")
    print("=" * 70)

    print("\nDataset Summary:")
    print(f"  Train samples: {len(fed_data.train_data)}")
    print(f"  Test samples: {len(fed_data.test_data)}")
    print(f"  Features: {fed_data.input_dim}")
    print(f"  Clients: {fed_data.n_clients}")
    print(f"  Non-IID alpha: {fed_data.alpha}")

    print("\nâœ… FederatedUNSWNB15 is ready for federated learning experiments!")
