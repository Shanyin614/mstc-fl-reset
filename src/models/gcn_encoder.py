# src/models/gcn_encoder.py
"""
Graph Convolutional Network (GCN) Encoder
ç”¨äºå°†å®¢æˆ·ç«¯ç‰¹å¾æ˜ å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´ï¼Œæ”¯æŒå±‚æ¬¡èšç±»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional
import numpy as np


class GCNLayer(nn.Module):
    """
    å•å±‚å›¾å·ç§¯å±‚

    å…¬å¼: H^(l+1) = Ïƒ(D^(-1/2) A D^(-1/2) H^(l) W^(l))
    å…¶ä¸­:
    - A: é‚»æ¥çŸ©é˜µï¼ˆå¸¦è‡ªç¯ï¼‰
    - D: åº¦çŸ©é˜µ
    - H: èŠ‚ç‚¹ç‰¹å¾
    - W: å¯å­¦ä¹ æƒé‡
    - Ïƒ: æ¿€æ´»å‡½æ•°
    """

    def __init__(self,
                 in_features: int,
                 out_features: int,
                 bias: bool = True,
                 dropout: float = 0.3):
        super(GCNLayer, self).__init__()

        self.in_features = in_features
        self.out_features = out_features

        # å¯å­¦ä¹ å‚æ•°
        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = nn.Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)

        self.dropout = nn.Dropout(dropout)

        # åˆå§‹åŒ–å‚æ•°
        self.reset_parameters()

    def reset_parameters(self):
        """Xavier åˆå§‹åŒ–"""
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(self,
                x: torch.Tensor,
                adj: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, in_features]
            adj: å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ [N, N]

        Returns:
            æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾ [N, out_features]
        """
        # 1. ç‰¹å¾å˜æ¢: X @ W
        x = self.dropout(x)
        support = torch.mm(x, self.weight)  # [N, out_features]

        # 2. å›¾å·ç§¯: A @ (X @ W)
        output = torch.spmm(adj, support) if adj.is_sparse else torch.mm(adj, support)

        # 3. åŠ åç½®
        if self.bias is not None:
            output = output + self.bias

        return output


class GCNEncoder(nn.Module):
    """
    å¤šå±‚ GCN ç¼–ç å™¨

    æ¶æ„:
    Input (n_features)
      â†’ GCN Layer 1 (hidden_dim) + ReLU + Dropout
      â†’ GCN Layer 2 (hidden_dim) + ReLU + Dropout
      â†’ GCN Layer 3 (embedding_dim)
      â†’ Output (ä½ç»´åµŒå…¥)
    """

    def __init__(self,
                 n_features: int,
                 hidden_dim: int = 128,
                 embedding_dim: int = 64,
                 n_layers: int = 3,
                 dropout: float = 0.3):
        super(GCNEncoder, self).__init__()

        self.n_features = n_features
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.n_layers = n_layers

        # æ„å»ºå¤šå±‚ GCN
        self.layers = nn.ModuleList()

        # è¾“å…¥å±‚
        self.layers.append(GCNLayer(n_features, hidden_dim, dropout=dropout))

        # éšè—å±‚
        for _ in range(n_layers - 2):
            self.layers.append(GCNLayer(hidden_dim, hidden_dim, dropout=dropout))

        # è¾“å‡ºå±‚ï¼ˆåµŒå…¥å±‚ï¼‰
        self.layers.append(GCNLayer(hidden_dim, embedding_dim, dropout=0.0))

        print(f"âœ… GCNEncoder initialized:")
        print(f"   Input: {n_features} features")
        print(f"   Hidden: {n_layers - 2} layers Ã— {hidden_dim} dims")
        print(f"   Output: {embedding_dim} dims")

    def forward(self,
                x: torch.Tensor,
                adj: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, n_features]
            adj: å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ [N, N]

        Returns:
            èŠ‚ç‚¹åµŒå…¥ [N, embedding_dim]
        """
        # é€å±‚ä¼ æ’­
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x, adj)
            x = F.relu(x)

        # æœ€åä¸€å±‚ï¼ˆä¸åŠ æ¿€æ´»ï¼‰
        x = self.layers[-1](x, adj)

        # L2 å½’ä¸€åŒ–ï¼ˆç”¨äºåç»­èšç±»ï¼‰
        x = F.normalize(x, p=2, dim=1)

        return x

    def get_embeddings(self,
                       features: torch.Tensor,
                       adj: torch.Tensor) -> torch.Tensor:
        """è·å–åµŒå…¥ï¼ˆæ¨ç†æ¨¡å¼ï¼‰"""
        self.eval()
        with torch.no_grad():
            embeddings = self.forward(features, adj)
        return embeddings


def normalize_adjacency(adj: torch.Tensor,
                        add_self_loop: bool = True) -> torch.Tensor:
    """
    å¯¹ç§°å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ

    å…¬å¼: A_norm = D^(-1/2) @ A @ D^(-1/2)

    Args:
        adj: åŸå§‹é‚»æ¥çŸ©é˜µ [N, N]
        add_self_loop: æ˜¯å¦æ·»åŠ è‡ªç¯

    Returns:
        å½’ä¸€åŒ–åçš„é‚»æ¥çŸ©é˜µ
    """
    if add_self_loop:
        # æ·»åŠ è‡ªç¯
        adj = adj + torch.eye(adj.size(0), device=adj.device)

    # è®¡ç®—åº¦çŸ©é˜µ
    degree = adj.sum(dim=1)

    # å¤„ç†å­¤ç«‹èŠ‚ç‚¹ï¼ˆåº¦ä¸º 0ï¼‰
    degree[degree == 0] = 1.0

    # D^(-1/2)
    degree_inv_sqrt = torch.pow(degree, -0.5)
    degree_inv_sqrt[torch.isinf(degree_inv_sqrt)] = 0.0

    # æ„å»ºå¯¹è§’çŸ©é˜µ
    degree_matrix = torch.diag(degree_inv_sqrt)

    # å¯¹ç§°å½’ä¸€åŒ–: D^(-1/2) @ A @ D^(-1/2)
    adj_normalized = torch.mm(torch.mm(degree_matrix, adj), degree_matrix)

    return adj_normalized


def sparse_mx_to_torch_sparse_tensor(sparse_mx):
    """å°† scipy ç¨€ç–çŸ©é˜µè½¬æ¢ä¸º PyTorch ç¨€ç–å¼ é‡"""
    sparse_mx = sparse_mx.tocoo().astype(np.float32)
    indices = torch.from_numpy(
        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)
    )
    values = torch.from_numpy(sparse_mx.data)
    shape = torch.Size(sparse_mx.shape)
    return torch.sparse.FloatTensor(indices, values, shape)


class GCNTrainer:
    """GCN è®­ç»ƒå™¨"""

    def __init__(self,
                 model: GCNEncoder,
                 lr: float = 0.01,
                 weight_decay: float = 5e-4):
        self.model = model
        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )

    def train_epoch(self,
                    features: torch.Tensor,
                    adj: torch.Tensor,
                    labels: torch.Tensor,
                    train_mask: torch.Tensor) -> float:
        """
        è®­ç»ƒä¸€ä¸ª epochï¼ˆèŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ï¼‰

        Args:
            features: èŠ‚ç‚¹ç‰¹å¾ [N, D]
            adj: é‚»æ¥çŸ©é˜µ [N, N]
            labels: èŠ‚ç‚¹æ ‡ç­¾ [N]
            train_mask: è®­ç»ƒé›†æ©ç  [N]

        Returns:
            è®­ç»ƒæŸå¤±
        """
        self.model.train()
        self.optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        embeddings = self.model(features, adj)

        # è®¡ç®—æŸå¤±ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šï¼‰
        loss = F.cross_entropy(
            embeddings[train_mask],
            labels[train_mask]
        )

        # åå‘ä¼ æ’­
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def evaluate(self,
                 features: torch.Tensor,
                 adj: torch.Tensor,
                 labels: torch.Tensor,
                 test_mask: torch.Tensor) -> Tuple[float, float]:
        """
        è¯„ä¼°æ¨¡å‹

        Returns:
            (loss, accuracy)
        """
        self.model.eval()

        with torch.no_grad():
            embeddings = self.model(features, adj)

            # æŸå¤±
            loss = F.cross_entropy(
                embeddings[test_mask],
                labels[test_mask]
            ).item()

            # å‡†ç¡®ç‡
            pred = embeddings[test_mask].argmax(dim=1)
            acc = (pred == labels[test_mask]).float().mean().item()

        return loss, acc


if __name__ == "__main__":
    # æµ‹è¯• GCN
    print("ğŸ§ª Testing GCN Encoder...")

    # æ¨¡æ‹Ÿæ•°æ®
    n_nodes = 10  # 10 ä¸ªå®¢æˆ·ç«¯
    n_features = 64  # ç‰¹å¾ç»´åº¦

    # éšæœºç‰¹å¾
    features = torch.randn(n_nodes, n_features)

    # éšæœºé‚»æ¥çŸ©é˜µï¼ˆç¨€ç–ï¼‰
    adj = torch.rand(n_nodes, n_nodes)
    adj = (adj > 0.7).float()  # 30% è¿æ¥
    adj = (adj + adj.T) / 2  # å¯¹ç§°åŒ–

    # å½’ä¸€åŒ–
    adj_norm = normalize_adjacency(adj)

    # åˆå§‹åŒ–ç¼–ç å™¨
    encoder = GCNEncoder(
        n_features=n_features,
        hidden_dim=32,
        embedding_dim=16,
        n_layers=3
    )

    # å‰å‘ä¼ æ’­
    embeddings = encoder(features, adj_norm)

    print(f"\nâœ… Test passed!")
    print(f"   Input shape:  {features.shape}")
    print(f"   Output shape: {embeddings.shape}")
    print(f"   Embeddings normalized: {torch.allclose(embeddings.norm(dim=1), torch.ones(n_nodes), atol=1e-5)}")
