# experiments/run_mstc_fl.py
"""
MSTC-FL ä¸»å®éªŒè„šæœ¬ï¼ˆæ–¹æ¡ˆ Aï¼šåŸç”Ÿ DataLoader å®ç°ï¼‰
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import sys
import os
from typing import Dict, List, Tuple

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.models.simple_ids import SimpleIDS
from src.features.adwin_scheduler_v2 import MultiClientADWIN
from src.features.adaptive_mstr import AdaptiveMSTR
from src.features.multiscale_dtw import MultiScaleDTW
from src.clustering.hierarchical_gcn import HierarchicalGCNClustering
from src.utils.unsw_nb15_loader import FederatedUNSWNB15


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience=5, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = None
        self.counter = 0
        self.early_stop = False

    def __call__(self, val_score):
        if self.best_score is None:
            self.best_score = val_score
        elif val_score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = val_score
            self.counter = 0

        return self.early_stop
# åœ¨ run_mstc_fl.py å¼€å¤´æ·»åŠ 
# class FocalLoss(nn.Module):
#     def __init__(self, alpha=0.25, gamma=2.0):
#         super().__init__()
#         self.alpha = alpha
#         self.gamma = gamma
#
#     def forward(self, inputs, targets):
#         ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
#         p_t = torch.exp(-ce_loss)
#         return (self.alpha * (1 - p_t) ** self.gamma * ce_loss).mean()
class FocalLoss(nn.Module):
    """
    Focal Loss - å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬ï¼ŒæŠ‘åˆ¶ç®€å•æ ·æœ¬çš„æŸå¤±

    å…¬å¼: FL(p_t) = -Î±(1-p_t)^Î³ * log(p_t)

    å‚æ•°:
        alpha: ç±»åˆ«æƒé‡ï¼ˆ0.25 è¡¨ç¤ºæ­£ç±»æƒé‡æ›´é«˜ï¼‰
        gamma: èšç„¦å‚æ•°ï¼ˆ2.0 è¡¨ç¤ºéš¾æ ·æœ¬æƒé‡æŒ‡æ•°çº§å¢åŠ ï¼‰
    """

    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        # âœ… éªŒè¯è¾“å…¥
        #print(f"[DEBUG] FocalLoss input: {inputs.shape}, target: {targets.shape}")
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')

        # è®¡ç®—é¢„æµ‹æ¦‚ç‡
        p_t = torch.exp(-ce_loss)

        # åº”ç”¨ Focal Loss å…¬å¼
        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class MSTCFL:
    """
    MSTC-FL ç³»ç»Ÿï¼ˆæ–¹æ¡ˆ Aï¼šå®Œæ•´ DataLoader å®ç°ï¼‰

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ PyTorch DataLoader è¿›è¡Œæ‰¹é‡è®­ç»ƒ
    - æ”¯æŒæ—¶åºèšåˆå’Œç‰¹å¾èšåˆ
    - å®Œæ•´çš„è®­ç»ƒ/è¯„ä¼°æµç¨‹
    """

    def __init__(self,
                 n_clients: int = 10,
                 n_ensemble: int = 6,
                 use_timeseries_clustering: bool = True,
                 learning_rate: float = 0.001,
                 local_epochs: int = 1,
                 batch_size: int = 64):
        """
        Args:
            n_clients: å®¢æˆ·ç«¯æ•°é‡
            n_ensemble: é›†æˆæˆå‘˜æ•°é‡
            use_timeseries_clustering: æ˜¯å¦ä½¿ç”¨æ—¶åºèšåˆ
            learning_rate: å­¦ä¹ ç‡
            local_epochs: æœ¬åœ°è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°
        """
        self.n_clients = n_clients
        self.n_ensemble = n_ensemble
        self.use_timeseries_clustering = use_timeseries_clustering
        self.learning_rate = learning_rate
        self.local_epochs = local_epochs
        self.batch_size = batch_size

        # æ¨¡å‹ç»„ä»¶
        self.ensemble = None
        self.global_model = None
        self.optimizers = None  # ä¸ºæ¯ä¸ª EM åˆ›å»ºä¼˜åŒ–å™¨

        # ADWIN++ å¤šå®¢æˆ·ç«¯ç®¡ç†å™¨
        self.multi_client_adwin = None

        # MSTR ç‰¹å¾æå–å™¨
        self.mstr_extractor = None

        # å¤šå°ºåº¦ DTW
        self.multiscale_dtw = None

        # å±‚æ¬¡ GCN èšç±»
        self.hierarchical_clustering = None

        # å­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯çš„æ—¶é—´åºåˆ—å†å²
        self.client_timeseries = {i: [] for i in range(n_clients)}

        # æ€§èƒ½å†å²
        self.em_history = []
        self.global_history = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.round_stats = []

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # âœ… ä¸ºæ¯ä¸ª EM åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.schedulers = None
        print(f"\nâœ… MSTC-FL initialized:")
        print(f"   - Clients: {n_clients}")
        print(f"   - Ensemble: {n_ensemble}")
        print(f"   - Clustering mode: {'Timeseries' if use_timeseries_clustering else 'Features'}")
        print(f"   - Learning rate: {learning_rate}")
        print(f"   - Local epochs: {local_epochs}")
        print(f"   - Batch size: {batch_size}")
        print(f"   - Device: {self.device}")




    def _init_models(self, input_dim: int):
        """å»¶è¿Ÿåˆå§‹åŒ–æ‰€æœ‰æ¨¡å—"""
        if self.ensemble is None:
            self.input_dim = input_dim

            print(f"\nğŸ”§ Initializing all modules...")

            # 1. IDS æ¨¡å‹å’Œä¼˜åŒ–å™¨
            self.ensemble = [SimpleIDS(input_dim).to(self.device) for _ in range(self.n_ensemble)]
            self.global_model = SimpleIDS(input_dim).to(self.device)

            # ä¸ºæ¯ä¸ª EM åˆ›å»ºä¼˜åŒ–å™¨
            self.optimizers = [
                optim.Adam(model.parameters(), lr=self.learning_rate)
                for model in self.ensemble
            ]

            print(f"   âœ“ IDS models: {self.n_ensemble} ensemble + 1 global")

            # 2. ADWIN++ å¤šå®¢æˆ·ç«¯ç®¡ç†å™¨
            self.multi_client_adwin = MultiClientADWIN(
                n_clients=self.n_clients,
                delta=0.05,
                min_window=5,
                max_window=100,
                sensitivity=1.0
            )
            print(f"   âœ“ ADWIN++ detectors: {self.n_clients} instances")

            # 3. MSTR ç‰¹å¾æå–å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not self.use_timeseries_clustering:
                self.mstr_extractor = AdaptiveMSTR(
                    base_scales={'micro': 5, 'meso': 15, 'macro': 40},
                    delta=0.002,
                    enable_adaptive=True
                )
                try:
                    feature_dim = self.mstr_extractor.get_feature_dim()
                except AttributeError:
                    feature_dim = len(self.mstr_extractor.scales) * 4
                print(f"   âœ“ MSTR feature extractor: {feature_dim} dims")
            else:
                feature_dim = self.n_clients
                print(f"   âœ“ MSTR feature extractor: Disabled (using timeseries)")

            # 4. å¤šå°ºåº¦ DTW
            self.multiscale_dtw = MultiScaleDTW(
                scales=[5, 10, 20],
                weights=[0.5, 0.3, 0.2],
                mode='timeseries' if self.use_timeseries_clustering else 'features'
            )
            print(f"   âœ“ MultiScaleDTW: 3 scales")

            # 5. å±‚æ¬¡ GCN èšç±»
            self.hierarchical_clustering = HierarchicalGCNClustering(
                n_clients=self.n_clients,
                feature_dim=feature_dim,
                embedding_dim=64,
                n_micro_clusters=5,
                n_meso_clusters=3,
                device=self.device
            )
            print(f"   âœ“ Hierarchical GCN clustering")
            # âœ… åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
            self.schedulers = [
                optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)
                for optimizer in self.optimizers
            ]
            print(f"\nâœ… All modules initialized (input_dim={input_dim})")

    def train_round(self, fed_data: FederatedUNSWNB15, round_idx: int):
        """
        å•è½®è®­ç»ƒæµç¨‹

        Args:
            fed_data: FederatedUNSWNB15 æ•°æ®å¯¹è±¡
            round_idx: å½“å‰è½®æ¬¡
        """
        print(f"\n{'=' * 70}")
        print(f"MSTC-FL - Round {round_idx}")
        print(f"{'=' * 70}\n")

        # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆä»ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ¨æ–­è¾“å…¥ç»´åº¦ï¼‰
        if self.ensemble is None:
            # è·å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
            temp_loader = fed_data.get_client_loader(0, batch_size=32)
            batch_x, _ = next(iter(temp_loader))
            self._init_models(batch_x.shape[1])

        # ==================== Step 1: å®¢æˆ·ç«¯è®­ç»ƒ ====================
        print("1ï¸âƒ£ Client Training...")
        client_accuracies = self._train_clients(fed_data)

        # æ›´æ–°æ¯ä¸ªå®¢æˆ·ç«¯çš„æ—¶é—´åºåˆ—
        for i, acc in enumerate(client_accuracies):
            self.client_timeseries[i].append(acc)

        # ==================== Step 2: æ¼‚ç§»æ£€æµ‹ ====================
        print(f"\n2ï¸âƒ£ Drift Detection (ADWIN++)...")
        drift_results = self._detect_drift(client_accuracies)

        # ==================== Step 3-4: è·ç¦»è®¡ç®— ====================
        if self.use_timeseries_clustering:
            print(f"\n3ï¸âƒ£ Multi-Scale DTW Distance (Timeseries Mode)...")
            dtw_distances = self._compute_multiscale_dtw_timeseries()
        else:
            print(f"\n3ï¸âƒ£ MSTR Feature Extraction...")
            mstr_features = self._extract_mstr_features()

            print(f"\n4ï¸âƒ£ Multi-Scale DTW Distance (Feature Mode)...")
            dtw_distances = self._compute_multiscale_dtw(mstr_features)

        # ==================== Step 5: GCN èšç±» ====================
        print(f"\n5ï¸âƒ£ Hierarchical GCN Clustering...")
        clustering_results = self._hierarchical_clustering(dtw_distances, round_idx)

        # ==================== Step 6-8: èšåˆä¸è¯„ä¼° ====================
        print(f"\n6ï¸âƒ£ Cluster-based Aggregation...")
        self._aggregate_models(clustering_results, client_accuracies)

        print(f"\n7ï¸âƒ£ Updating Ensemble Members...")
        self._update_ensemble(client_accuracies)

        print(f"\n8ï¸âƒ£ Evaluation...")
        round_metrics = self._evaluate_round(fed_data, round_idx, drift_results, clustering_results)

        self.round_stats.append(round_metrics)

        self._print_round_summary(round_idx, round_metrics)

        print(f"\nâœ… Round {round_idx} completed!")

    def _train_clients(self, fed_data: FederatedUNSWNB15) -> List[float]:
        """å®¢æˆ·ç«¯è®­ç»ƒï¼ˆä½¿ç”¨ Focal Lossï¼‰"""
        client_accuracies = []

        for i in range(self.n_clients):
            print(f"  Client {i}...", end=" ")

            em_idx = i % self.n_ensemble
            em_model = self.ensemble[em_idx]
            optimizer = self.optimizers[em_idx]
            scheduler = self.schedulers[em_idx]
            client_loader = fed_data.get_client_loader(i, batch_size=self.batch_size)
            # âœ… æ ¹æ®å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒåŠ¨æ€è°ƒæ•´
            attack_ratio = fed_data.get_client_attack_ratio(i)

            if attack_ratio < 0.2:  # Client 1, 8 (å‡ ä¹æ— æ”»å‡»)
                alpha, gamma = 0.50, 3.0  # å¤§å¹…æé«˜æ”»å‡»ç±»æƒé‡
            elif attack_ratio > 0.8:  # Client 0, 6, 9 (é«˜æ”»å‡»ç‡)
                alpha, gamma = 0.10, 3.0  # æé«˜æ­£å¸¸ç±»æƒé‡
            else:
                alpha, gamma = 0.25, 2.5

            criterion = FocalLoss(alpha=alpha, gamma=gamma)

            print(f"  Client {i} (Attack={attack_ratio:.2f})...", end=" ")
            em_model.train()

            # âœ… æ›¿æ¢æŸå¤±å‡½æ•°
            criterion = FocalLoss(alpha=0.25, gamma=2.0)  # åŸ: nn.CrossEntropyLoss()

            total_correct = 0
            total_samples = 0
            total_loss = 0.0

            for epoch in range(self.local_epochs):
                for batch_x, batch_y in client_loader:
                    if batch_x.size(0) == 0:
                        continue

                    batch_x = batch_x.to(self.device)

                    if batch_y.dim() > 1:
                        batch_y = batch_y.squeeze(1)
                    batch_y = batch_y.to(self.device)

                    if batch_x.size(0) != batch_y.size(0):
                        continue

                    optimizer.zero_grad()
                    outputs = em_model(batch_x)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()

                    _, predicted = torch.max(outputs, 1)
                    total_correct += (predicted == batch_y).sum().item()
                    total_samples += batch_y.size(0)
                    total_loss += loss.item()

            acc = total_correct / total_samples if total_samples > 0 else 0.0
            avg_loss = total_loss / max(len(client_loader) * self.local_epochs, 1)
            scheduler.step()
            client_accuracies.append(acc)
            print(f"Acc={acc:.3f}, Loss={avg_loss:.4f}, LR={scheduler.get_last_lr()[0]:.6f} âœ“")

        return client_accuracies

    ##def _detect_drift(self, client_accuracies: List[float]) -> Dict[int, bool]:
        # """ADWIN++ æ¼‚ç§»æ£€æµ‹"""
        # drift_results = {}
        # n_drifts = 0
        #
        # for i, acc in enumerate(client_accuracies):
        #     has_drift = self.multi_client_adwin.update(i, acc)
        #     drift_results[i] = has_drift
        #     if has_drift:
        #         n_drifts += 1
        #
        # print(f"  Detected {n_drifts}/{self.n_clients} clients with drift")
        #
        # if n_drifts > 0:
        #     drift_clients = [i for i, has_drift in drift_results.items() if has_drift]
        #     print(f"  Drifted clients: {drift_clients}")
        #
        # return drift_results
        # experiments/run_mstc_fl.py

    def _detect_drift(self, client_accuracies: List[float]) -> Dict[int, bool]:
        """ADWIN++ æ¼‚ç§»æ£€æµ‹ï¼ˆæ‰¹é‡æ›´æ–°ï¼‰"""
        result = self.multi_client_adwin.update(client_accuracies)

        drift_results = {i: False for i in range(self.n_clients)}
        for client_id in result['drift_clients']:
            drift_results[client_id] = True

        n_drifts = result['n_drifts']
        print(f"  Detected {n_drifts}/{self.n_clients} clients with drift")

        if n_drifts > 0:
            print(f"  Drifted clients: {result['drift_clients']}")

        return drift_results

    def _compute_multiscale_dtw_timeseries(self) -> np.ndarray:
        """ä»å®¢æˆ·ç«¯æ—¶é—´åºåˆ—è®¡ç®— DTW è·ç¦»"""
        # å°†åˆ—è¡¨è½¬æ¢ä¸º numpy æ•°ç»„
        client_timeseries_np = {
            i: np.array(ts) for i, ts in self.client_timeseries.items()
        }

        # è®¡ç®— DTW è·ç¦»
        dtw_distances = self.multiscale_dtw.compute_distance_matrix_from_client_timeseries(
            client_timeseries_np
        )

        print(f"  DTW matrix: {dtw_distances.shape}")
        if dtw_distances[dtw_distances > 0].size > 0:
            print(f"  Distance range: [{dtw_distances[dtw_distances > 0].min():.4f}, {dtw_distances.max():.4f}]")

        return dtw_distances

    def _compute_multiscale_dtw(self, features: np.ndarray) -> np.ndarray:
        """ç‰¹å¾èšåˆæ¨¡å¼çš„ DTW è®¡ç®—"""
        dtw_distances = self.multiscale_dtw.compute_distance_matrix(features)
        print(f"  DTW matrix: {dtw_distances.shape}")
        return dtw_distances

    def _extract_mstr_features(self) -> np.ndarray:
        """MSTR ç‰¹å¾æå–ï¼ˆä»…åœ¨ç‰¹å¾èšåˆæ¨¡å¼ï¼‰"""
        em_history = np.array(self.em_history)

        if len(em_history) == 0:
            print(f"  MSTR features: ({self.n_clients}, 16) (random initialization)")
            return np.random.randn(self.n_clients, 16)

        features_list = []
        for i in range(self.n_clients):
            em_idx = i % self.n_ensemble
            client_em_history = em_history[:, em_idx].reshape(-1, 1)

            if len(client_em_history) < 5:
                client_features = np.random.randn(16)
            else:
                current_variance = np.var(client_em_history[-5:])
                client_features = self.mstr_extractor.extract_features_adaptive(
                    client_em_history,
                    current_variance
                )

            features_list.append(client_features)

        features = np.array(features_list)
        print(f"  MSTR features: {features.shape}")

        return features

    def _hierarchical_clustering(self, dtw_distances: np.ndarray, round_idx: int) -> Dict:
        """å±‚æ¬¡ GCN èšç±»"""
        # æ„å»ºé‚»æ¥çŸ©é˜µï¼ˆè·ç¦»çš„å€’æ•°ï¼Œé¿å…é™¤é›¶ï¼‰
        epsilon = 1e-6
        adjacency = 1.0 / (dtw_distances + epsilon)
        np.fill_diagonal(adjacency, 0)

        # å½’ä¸€åŒ–
        adjacency = adjacency / (adjacency.sum(axis=1, keepdims=True) + epsilon)

        # æ‰§è¡Œèšç±»
        clustering_results = self.hierarchical_clustering.cluster(
            distance_matrix=dtw_distances,
            adjacency_matrix=adjacency
        )

        # æ‰“å°èšç±»ä¿¡æ¯
        print(f"  Micro clusters: {len(clustering_results['micro_clusters'])}")
        for cluster_id, members in clustering_results['micro_clusters'].items():
            print(f"    Cluster {cluster_id}: {members}")

        return clustering_results

    def _aggregate_models(self, clustering_results: Dict, client_accuracies: List[float]):
        """åŸºäºèšç±»çš„æ¨¡å‹èšåˆ"""
        micro_clusters = clustering_results['micro_clusters']

        # é‡ç½®å…¨å±€æ¨¡å‹å‚æ•°
        global_dict = self.global_model.state_dict()
        for key in global_dict.keys():
            global_dict[key] = torch.zeros_like(global_dict[key])

        total_weight = 0.0

        for cluster_id, client_ids in micro_clusters.items():
            # è®¡ç®—ç°‡å†…å¹³å‡å‡†ç¡®ç‡
            cluster_accs = [client_accuracies[i] for i in client_ids]
            cluster_weight = np.mean(cluster_accs) * len(client_ids)

            # èšåˆç°‡å†…å®¢æˆ·ç«¯çš„æ¨¡å‹
            for client_id in client_ids:
                em_idx = client_id % self.n_ensemble
                em_dict = self.ensemble[em_idx].state_dict()

                for key in global_dict.keys():
                    global_dict[key] += em_dict[key] * cluster_weight / len(client_ids)

            total_weight += cluster_weight

        # å½’ä¸€åŒ–
        if total_weight > 0:
            for key in global_dict.keys():
                global_dict[key] /= total_weight

        self.global_model.load_state_dict(global_dict)

        print(f"  Aggregated {len(micro_clusters)} clusters")

    def _update_ensemble(self, client_accuracies: List[float]):
        """æ›´æ–°é›†æˆæˆå‘˜"""
        # è®¡ç®—æ¯ä¸ª EM çš„å¹³å‡æ€§èƒ½
        em_performances = []
        for em_idx in range(self.n_ensemble):
            client_ids = [i for i in range(self.n_clients) if i % self.n_ensemble == em_idx]
            em_acc = np.mean([client_accuracies[i] for i in client_ids])
            em_performances.append(em_acc)

        self.em_history.append(em_performances)

        # æ‰¾å‡ºè¡¨ç°æœ€å·®çš„ EM
        worst_em_idx = np.argmin(em_performances)

        # ç”¨å…¨å±€æ¨¡å‹æ›´æ–°è¡¨ç°æœ€å·®çš„ EM
        self.ensemble[worst_em_idx].load_state_dict(self.global_model.state_dict())

        print(f"  EM performances: {[f'{acc:.3f}' for acc in em_performances]}")
        print(f"  Updated EM {worst_em_idx} (worst: {em_performances[worst_em_idx]:.3f})")

    def _evaluate_round(
            self,
            fed_data: FederatedUNSWNB15,
            round_idx: int,
            drift_results: Dict,
            clustering_results: Dict
    ) -> Dict:
        """
        è¯„ä¼°å½“å‰è½®æ¬¡ï¼ˆä¿®å¤é˜ˆå€¼æœç´¢ + åŠ é€Ÿä¼˜åŒ–ï¼‰
        """
        from sklearn.metrics import (
            accuracy_score, f1_score, precision_score, recall_score,
            roc_curve, confusion_matrix
        )

        test_loader = fed_data.get_test_loader(batch_size=128)

        self.global_model.eval()

        # åˆå§‹åŒ–
        all_probs = []
        all_labels = []

        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                if batch_x.size(0) == 0:
                    continue

                batch_x = batch_x.to(self.device)

                if batch_y.dim() > 1:
                    batch_y = batch_y.squeeze(1)
                batch_y = batch_y.to(self.device)

                outputs = self.global_model(batch_x)
                probs = torch.softmax(outputs, dim=1)[:, 1]

                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(batch_y.cpu().numpy())

        # éªŒè¯æ•°æ®æ”¶é›†
        if len(all_probs) == 0 or len(all_labels) == 0:
            print("\nâš ï¸  Warning: No utils collected during evaluation!")
            return {
                'round': round_idx,
                'global_acc': 0.0,
                'global_f1': 0.0,
                'global_precision': 0.0,
                'global_recall': 0.0,
                'n_drifts': sum(drift_results.values()) if isinstance(drift_results, dict) else 0,
                'n_clusters': len(clustering_results.get('micro_clusters', []))
            }

        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)

        # âœ… ä¿®å¤ 1ï¼šæ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        n_positive = (all_labels == 1).sum()
        n_negative = (all_labels == 0).sum()

        print(f"\n  Test set distribution:")
        print(f"    Positive (Attack): {n_positive} ({n_positive / len(all_labels) * 100:.2f}%)")
        print(f"    Negative (Normal): {n_negative} ({n_negative / len(all_labels) * 100:.2f}%)")

        # âœ… ä¿®å¤ 2ï¼šä¼˜åŒ–é˜ˆå€¼æœç´¢ï¼ˆå¿«é€Ÿç‰ˆï¼‰
        try:
            fpr, tpr, thresholds = roc_curve(all_labels, all_probs)

            # å‡å°‘æœç´¢ç©ºé—´ï¼ˆåªæœç´¢ 100 ä¸ªå€™é€‰é˜ˆå€¼ï¼‰
            if len(thresholds) > 100:
                indices = np.linspace(0, len(thresholds) - 1, 100, dtype=int)
                thresholds = thresholds[indices]

            print(f"  Searching {len(thresholds)} thresholds...")

            best_threshold = 0.5
            best_f1 = 0.0

            # âœ… ä½¿ç”¨ F1-score è€Œä¸æ˜¯ Precision-Recall å·®å€¼
            for thresh in thresholds:
                # è·³è¿‡æç«¯å€¼
                if thresh == np.inf or thresh == -np.inf or np.isnan(thresh):
                    continue

                preds = (all_probs >= thresh).astype(int)

                # è·³è¿‡å…¨ 0 æˆ–å…¨ 1 é¢„æµ‹
                if preds.sum() == 0 or preds.sum() == len(preds):
                    continue

                f1 = f1_score(all_labels, preds, average='binary', zero_division=0)

                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = thresh

            print(f"  âœ… Best threshold: {best_threshold:.4f} (F1={best_f1:.4f})")

            # ä½¿ç”¨æœ€ä½³é˜ˆå€¼é‡æ–°é¢„æµ‹
            all_preds = (all_probs >= best_threshold).astype(int)

        except Exception as e:
            print(f"\nâš ï¸  Warning: Threshold search failed ({e})")
            print("  Using default threshold 0.5")
            best_threshold = 0.5
            all_preds = (all_probs >= best_threshold).astype(int)

        # âœ… è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        global_acc = accuracy_score(all_labels, all_preds)
        global_f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)
        global_precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
        global_recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)

        # âœ… æ‰“å°æ··æ·†çŸ©é˜µï¼ˆå¸®åŠ©è¯Šæ–­ï¼‰
        tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()
        print(f"\n  Confusion Matrix:")
        print(f"    TN={tn:6d}  FP={fp:6d}")
        print(f"    FN={fn:6d}  TP={tp:6d}")

        metrics = {
            'round': round_idx,
            'global_acc': global_acc,
            'global_f1': global_f1,
            'global_precision': global_precision,
            'global_recall': global_recall,
            'n_drifts': sum(drift_results.values()) if isinstance(drift_results, dict) else 0,
            'n_clusters': len(clustering_results.get('micro_clusters', []))
        }

        self.global_history.append(global_acc)

        return metrics

    def _print_round_summary(self, round_idx: int, metrics: Dict):
        """æ‰“å°è½®æ¬¡æ‘˜è¦"""
        print(f"\n{'=' * 70}")
        print(f"Round {round_idx} Summary")
        print(f"{'=' * 70}")
        print(f"  Global Accuracy:  {metrics['global_acc']:.4f}")
        print(f"  Global F1 Score:  {metrics['global_f1']:.4f}")
        print(f"  Precision:        {metrics['global_precision']:.4f}")
        print(f"  Recall:           {metrics['global_recall']:.4f}")
        print(f"  Detected Drifts:  {metrics['n_drifts']}")
        print(f"  Active Clusters:  {metrics['n_clusters']}")
        print(f"{'=' * 70}")

    def run_experiment(self, fed_data: FederatedUNSWNB15, n_rounds: int = 50) -> Dict:
        """
        è¿è¡Œå®Œæ•´å®éªŒ

        Args:
            fed_data: FederatedUNSWNB15 æ•°æ®å¯¹è±¡
            n_rounds: è®­ç»ƒè½®æ•°

        Returns:
            history: è®­ç»ƒå†å²
        """
        print(f"\nğŸš€ Starting MSTC-FL experiment: {n_rounds} rounds\n")
        early_stopping = EarlyStopping(patience=5, min_delta=0.01)
        for round_idx in range(n_rounds):
            self.train_round(fed_data, round_idx)
            if early_stopping(self.global_history[-1]):
                print(f"\nâ¹ï¸  Early stopping at round {round_idx}")
                print(f"   Best score: {early_stopping.best_score:.4f}")
                break
        print(f"\nâœ… Experiment completed!")

        # è¿”å›è®­ç»ƒå†å²
        history = {
            'round_stats': self.round_stats,
            'global_history': self.global_history,
            'em_history': self.em_history
        }

        return history


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print("ğŸš€ Starting MSTC-FL Experiment (Method A: Native DataLoader)...\n")
    from pathlib import Path

    ROOT = Path(__file__).resolve().parents[1]  # é¡¹ç›®æ ¹ï¼ˆâ€¦/MSTC-FLï¼‰
    DATA = ROOT / "data" / "unsw"
    train_path = str(DATA / "UNSW_NB15_training-set.csv")
    test_path = str(DATA / "UNSW_NB15_testing-set.csv")
    # åŠ è½½æ•°æ®é›†
    fed_data = FederatedUNSWNB15(
        train_path=train_path,
        test_path=test_path,
        n_clients=10,
        alpha=0.5  # Non-IID ç¨‹åº¦
    )

    # åˆ›å»º MSTC-FL å®ä¾‹
    mstc_fl = MSTCFL(
        n_clients=10,
        n_ensemble=5,
        use_timeseries_clustering=True,  # ä½¿ç”¨æ—¶åºèšåˆ
        learning_rate=0.001,
        local_epochs=3,
        batch_size=128
    )

    # å¿«é€Ÿæµ‹è¯•ï¼š3 è½®
    # print(f"\nğŸ§ª Quick test: 3 rounds...\n")
    # for round_idx in range(3):
    #     mstc_fl.train_round(fed_data, round_idx)
    #
    # print("\nâœ… Quick test completed!")

    #å®Œæ•´è®­ç»ƒï¼ˆå–æ¶ˆæ³¨é‡Šè¿è¡Œ 50 è½®ï¼‰
    history = mstc_fl.run_experiment(fed_data, n_rounds=25)

    # ä¿å­˜ç»“æœ
    import pickle
    with open('results/mstc_fl_history.pkl', 'wb') as f:
         pickle.dump(history, f)
    print("\nâœ… Results saved to results/mstc_fl_25rounds.pkl")