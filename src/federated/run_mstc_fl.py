# experiments/run_mstc_fl.py
"""
MSTC-FL ä¸»å®éªŒè„šæœ¬ï¼ˆæ–¹æ¡ˆ Aï¼šåŸç”Ÿ DataLoader å®ç°ï¼‰
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import sys
import os
from typing import Dict, List, Tuple

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..','..')))

from src.models.simple_ids import SimpleIDS
from src.features.adwin_scheduler_v2 import MultiClientADWIN
from src.features.adaptive_mstr import AdaptiveMSTR
from src.features.multiscale_dtw import MultiScaleDTW
from src.clustering.hierarchical_gcn import HierarchicalGCNClustering
from src.utils.unsw_nb15_loader import FederatedUNSWNB15
from src.core.early_stopping import EarlyStopping
from src.core.losses import FocalLoss
from src.federated.client_trainer import ClientTrainer
from src.federated.evaluator import ModelEvaluator
from src.federated.model_aggregator import ModelAggregator
from src.federated.ensemble_manager import EnsembleManager
from src.federated.workflow_manager import WorkflowManager
from src.aggregation.hierarchical_aggregator import HierarchicalAggregator
class MSTCFL:
    """
    MSTC-FL ç³»ç»Ÿï¼ˆæ–¹æ¡ˆ Aï¼šå®Œæ•´ DataLoader å®ç°ï¼‰

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ PyTorch DataLoader è¿›è¡Œæ‰¹é‡è®­ç»ƒ
    - æ”¯æŒæ—¶åºèšåˆå’Œç‰¹å¾èšåˆ
    - å®Œæ•´çš„è®­ç»ƒ/è¯„ä¼°æµç¨‹
    - ä¸‰å±‚å±‚æ¬¡èšç±»ï¼ˆMicro + Meso + Macroï¼‰
    """

    def __init__(self,
                 n_clients: int = 10,
                 n_ensemble: int = 6,
                 use_timeseries_clustering: bool = True,
                 learning_rate: float = 0.001,
                 local_epochs: int = 1,
                 batch_size: int = 64):
        """
        Args:
            n_clients: å®¢æˆ·ç«¯æ•°é‡
            n_ensemble: é›†æˆæˆå‘˜æ•°é‡
            use_timeseries_clustering: æ˜¯å¦ä½¿ç”¨æ—¶åºèšåˆ
            learning_rate: å­¦ä¹ ç‡
            local_epochs: æœ¬åœ°è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°
        """
        self.n_clients = n_clients
        self.n_ensemble = n_ensemble
        self.use_timeseries_clustering = use_timeseries_clustering
        self.learning_rate = learning_rate
        self.local_epochs = local_epochs
        self.batch_size = batch_size

        # ========== æ¨¡å‹ç»„ä»¶ ==========
        self.ensemble = None
        self.global_model = None
        self.optimizers = None
        self.schedulers = None

        # ========== å­˜å‚¨ ==========
        self.client_timeseries = {i: [] for i in range(n_clients)}
        self.em_history = []
        self.global_history = []
        self.round_stats = []

        # ========== è®¾å¤‡ ==========
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ========== æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ==========

        # 1ï¸âƒ£ å®¢æˆ·ç«¯è®­ç»ƒå™¨
        self.client_trainer = ClientTrainer(
            n_clients=n_clients,
            n_ensemble=n_ensemble,
            local_epochs=local_epochs,
            batch_size=batch_size,
            device=self.device
        )

        # 2ï¸âƒ£ é›†æˆç®¡ç†å™¨
        self.ensemble_manager = EnsembleManager(
            n_clients=n_clients,
            n_ensemble=n_ensemble
        )

        # 3ï¸âƒ£ æ¨¡å‹èšåˆå™¨
        self.model_aggregator = ModelAggregator(
            n_clients=n_clients,
            n_ensemble=n_ensemble,
            device=self.device
        )

        # 4ï¸âƒ£ å·¥ä½œæµç®¡ç†å™¨
        self.workflow_manager = WorkflowManager(
            n_clients=n_clients,
            use_timeseries_clustering=use_timeseries_clustering,
            device=self.device
        )

        # 5ï¸âƒ£ æ¨¡å‹è¯„ä¼°å™¨
        self.evaluator = ModelEvaluator(device=self.device)

        # 6ï¸âƒ£ ä¸‰å±‚èšåˆå™¨
        self.hierarchical_aggregator = HierarchicalAggregator(
            n_clients=n_clients
        )

        # âœ… ä¾èµ–æ³¨å…¥ï¼šå°†ç»„ä»¶æ³¨å…¥åˆ° workflow_manager
        self.workflow_manager.set_components(
            client_trainer=self.client_trainer,
            model_aggregator=self.model_aggregator,
            ensemble_manager=self.ensemble_manager,
            evaluator=self.evaluator
        )

        # ========== å†å²è®°å½• ==========
        self.history = {
            'micro_clusters': [],
            'meso_labels': [],
            'macro_labels': [],
            'client_accuracies': [],
            'round_stats': [],
            'drift_history': []
        }

        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        print(f"\nâœ… MSTC-FL initialized:")
        print(f"   - Clients: {self.n_clients}")
        print(f"   - Ensemble: {self.n_ensemble}")
        print(f"   - Clustering mode: {'Timeseries' if self.use_timeseries_clustering else 'Features'}")
        print(f"   - Learning rate: {self.learning_rate}")
        print(f"   - Local epochs: {self.local_epochs}")
        print(f"   - Batch size: {self.batch_size}")
        print(f"   - Device: {self.device}")

    # ========================================
    # æ¨¡å‹åˆå§‹åŒ–
    # ========================================

    def _init_models(self, fed_data: FederatedUNSWNB15):
        """
        å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹ï¼ˆä»…åœ¨ç¬¬ä¸€è½®è®­ç»ƒæ—¶è°ƒç”¨ï¼‰

        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
        """
        if self.ensemble is None:
            input_dim = fed_data.input_dim

            print(f"\nğŸ”§ Initializing models...")

            # 1ï¸âƒ£ IDS æ¨¡å‹
            self.ensemble = [
                SimpleIDS(input_dim).to(self.device)
                for _ in range(self.n_ensemble)
            ]
            self.global_model = SimpleIDS(input_dim).to(self.device)

            # 2ï¸âƒ£ ä¼˜åŒ–å™¨
            self.optimizers = [
                optim.Adam(model.parameters(), lr=self.learning_rate)
                for model in self.ensemble
            ]

            # 3ï¸âƒ£ å­¦ä¹ ç‡è°ƒåº¦å™¨
            self.schedulers = [
                optim.lr_scheduler.CosineAnnealingLR(
                    optimizer, T_max=50, eta_min=1e-5
                )
                for optimizer in self.optimizers
            ]

            print(f"   âœ“ IDS models: {self.n_ensemble} ensemble + 1 global")
            print(f"   âœ“ Optimizers: {self.n_ensemble} Adam")
            print(f"   âœ“ Schedulers: {self.n_ensemble} CosineAnnealing")

            # 4ï¸âƒ£ æ‰“å° workflow_manager å·²åˆå§‹åŒ–çš„æ¨¡å—
            feature_dim = self.n_clients if self.use_timeseries_clustering else 16
            self.workflow_manager.init_modules(feature_dim)

            print(f"\nâœ… All modules initialized (input_dim={input_dim})")

    # ========================================
    # è¾…åŠ©æ–¹æ³•
    # ========================================

    def _extract_micro_clusters(self, micro_labels: np.ndarray) -> Dict[int, List[int]]:
        """
        å°† Micro æ ‡ç­¾æ•°ç»„è½¬æ¢ä¸ºç°‡å­—å…¸

        Args:
            micro_labels: Micro èšç±»æ ‡ç­¾ [N]

        Returns:
            {cluster_id: [client_ids]}
        """
        micro_clusters = {}
        for cluster_id in np.unique(micro_labels):
            members = np.where(micro_labels == cluster_id)[0].tolist()
            micro_clusters[int(cluster_id)] = members
        return micro_clusters

    # ========================================
    # è®­ç»ƒæµç¨‹
    # ========================================

    def train_round(self, fed_data: FederatedUNSWNB15, round_id: int) -> Dict:
        """
        è®­ç»ƒä¸€è½®

        Args:
            fed_data: è”é‚¦æ•°æ®å¯¹è±¡
            round_id: å½“å‰è½®æ¬¡ï¼ˆä» 0 å¼€å§‹ï¼‰

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        print(f"\n{'=' * 70}")
        print(f"Round {round_id + 1}")
        print(f"{'=' * 70}")

        # âœ… å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹
        if self.ensemble is None:
            self._init_models(fed_data)

        # 1ï¸âƒ£ å®¢æˆ·ç«¯è®­ç»ƒï¼ˆâœ… å§”æ‰˜ç»™ workflow_managerï¼‰
        print("\n1ï¸âƒ£ Training clients...")
        client_models, client_accuracies = self.workflow_manager.train_clients(
            fed_data=fed_data,
            ensemble=self.ensemble,
            optimizers=self.optimizers,
            schedulers=self.schedulers,
            client_timeseries=self.client_timeseries
        )

        # 2ï¸âƒ£ æ¼‚ç§»æ£€æµ‹
        print("\n2ï¸âƒ£ Drift detection (ADWIN++)...")
        drift_results = self.workflow_manager.detect_drift(client_accuracies)

        # 3ï¸âƒ£ DTW è·ç¦»è®¡ç®—
        print("\n3ï¸âƒ£ Computing DTW distances...")
        dtw_distances = self.workflow_manager.compute_dtw_distances(
            client_timeseries=self.client_timeseries
        )

        # 4ï¸âƒ£ ä¸‰å±‚èšç±»
        print("\n4ï¸âƒ£ Hierarchical clustering...")
        clustering_results = self.workflow_manager.perform_clustering(
            dtw_distances,
            round_idx=round_id
        )

        # âœ… æå–ä¸‰å±‚ç»“æœ
        micro_labels = clustering_results['micro']['labels']
        meso_labels = clustering_results['meso']['labels']
        macro_labels = (
            clustering_results['macro']['labels']
            if clustering_results['macro'] is not None
            else meso_labels
        )

        # âœ… è½¬æ¢ä¸ºç°‡å­—å…¸
        micro_clusters = self._extract_micro_clusters(micro_labels)

        # 5ï¸âƒ£ ä¸‰å±‚èšåˆ
        print("\n5ï¸âƒ£ Hierarchical aggregation...")
        cluster_representatives = self.hierarchical_aggregator.aggregate(
            client_models=client_models,
            client_accuracies=client_accuracies,
            micro_clusters=micro_clusters,
            meso_labels=meso_labels,
            macro_labels=macro_labels,
            round_id=round_id
        )

        # 6ï¸âƒ£ æ›´æ–°é›†æˆï¼ˆâœ… å§”æ‰˜ç»™ workflow_managerï¼‰
        print("\n6ï¸âƒ£ Updating ensemble...")
        self.workflow_manager.update_ensemble(
            cluster_representatives=cluster_representatives,
            client_accuracies=client_accuracies,
            micro_clusters=micro_clusters,
            ensemble=self.ensemble
        )

        # 7ï¸âƒ£ å…¨å±€è¯„ä¼°ï¼ˆâœ… å§”æ‰˜ç»™ workflow_managerï¼‰
        print("\n7ï¸âƒ£ Global evaluation...")
        metrics = self.workflow_manager.global_evaluation(
            fed_data=fed_data,
            ensemble=self.ensemble,
            n_clients=self.n_clients,
            global_history=self.global_history,
            round_idx=round_id,
            drift_results=drift_results,
            clustering_results=clustering_results
        )

        # âœ… è®°å½•å†å²
        self.history['micro_clusters'].append(micro_clusters)
        self.history['meso_labels'].append(meso_labels)
        self.history['macro_labels'].append(macro_labels)
        self.history['client_accuracies'].append(client_accuracies)
        self.history['drift_history'].append(drift_results)
        self.history['round_stats'].append({
            'round': round_id + 1,
            'global_acc': metrics['accuracy'],
            'global_f1': metrics['f1_score'],
            'global_precision': metrics['precision'],
            'global_recall': metrics['recall'],
            'n_micro_clusters': len(micro_clusters),
            'n_meso_clusters': len(np.unique(meso_labels)),
            'n_macro_clusters': len(np.unique(macro_labels)),
            'n_drifts': drift_results['n_drifts']
        })

        # âœ… åŒæ­¥åˆ°åŸæœ‰çš„ round_statsï¼ˆä¿æŒå…¼å®¹ï¼‰
        self.round_stats.append(self.history['round_stats'][-1])

        # âœ… æ‰“å°ä¸‰å±‚ä¿¡æ¯
        print(f"\n{'=' * 70}")
        print(f"Round {round_id + 1} Summary:")
        print(f"  Global Accuracy:  {metrics['accuracy']:.4f}")
        print(f"  Global F1 Score:  {metrics['f1_score']:.4f}")
        print(f"  Micro Clusters:   {len(micro_clusters)}")
        print(f"  Meso Clusters:    {len(np.unique(meso_labels))}")
        print(f"  Macro Clusters:   {len(np.unique(macro_labels))}")
        print(f"  Detected Drifts:  {drift_results['n_drifts']}")
        print(f"{'=' * 70}\n")

        return metrics

    # ========================================
    # å®éªŒè¿è¡Œ
    # ========================================

    def run_experiment(self, fed_data: FederatedUNSWNB15, n_rounds: int = 50) -> Dict:
        """
        è¿è¡Œå®Œæ•´å®éªŒ

        Args:
            fed_data: è”é‚¦æ•°æ®å¯¹è±¡
            n_rounds: è®­ç»ƒè½®æ•°

        Returns:
            å®éªŒå†å²å­—å…¸
        """
        print(f"\nğŸš€ Starting MSTC-FL experiment: {n_rounds} rounds\n")

        early_stopping = EarlyStopping(patience=5, min_delta=0.01)

        for round_idx in range(n_rounds):
            # âœ… ä¿®æ­£ï¼šå‚æ•°é¡ºåºä¸º (fed_data, round_id)
            self.train_round(fed_data, round_idx)

            # æ—©åœæ£€æŸ¥
            if len(self.global_history) > 0:
                if early_stopping(self.global_history[-1]):
                    print(f"\nâ¹ï¸  Early stopping at round {round_idx + 1}")
                    print(f"   Best score: {early_stopping.best_score:.4f}")
                    break

        print(f"\nâœ… Experiment completed!")

        return {
            'round_stats': self.round_stats,
            'global_history': self.global_history,
            'em_history': self.ensemble_manager.get_history(),
            'history': self.history  # âœ… æ–°å¢ï¼šè¿”å›å®Œæ•´å†å²
        }

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print("ğŸš€ Starting MSTC-FL Experiment (Method A: Native DataLoader)...\n")
    from pathlib import Path

    ROOT = Path(__file__).resolve().parents[2]  # é¡¹ç›®æ ¹ï¼ˆâ€¦/MSTC-FLï¼‰
    DATA = ROOT / "data" / "unsw"
    train_path = str(DATA / "UNSW_NB15_training-set.csv")
    test_path = str(DATA / "UNSW_NB15_testing-set.csv")
    # åŠ è½½æ•°æ®é›†
    fed_data = FederatedUNSWNB15(
        train_path=train_path,
        test_path=test_path,
        n_clients=10,
        alpha=0.5  # Non-IID ç¨‹åº¦
    )

    # åˆ›å»º MSTC-FL å®ä¾‹
    mstc_fl = MSTCFL(
        n_clients=10,
        n_ensemble=5,
        use_timeseries_clustering=True,  # ä½¿ç”¨æ—¶åºèšåˆ
        learning_rate=0.001,
        local_epochs=3,
        batch_size=128
    )

    # å¿«é€Ÿæµ‹è¯•ï¼š3 è½®
    # print(f"\nğŸ§ª Quick test: 3 rounds...\n")
    # for round_idx in range(3):
    #     mstc_fl.train_round(fed_data, round_idx)
    #
    # print("\nâœ… Quick test completed!")

    #å®Œæ•´è®­ç»ƒï¼ˆå–æ¶ˆæ³¨é‡Šè¿è¡Œ 50 è½®ï¼‰
    history = mstc_fl.run_experiment(fed_data, n_rounds=25)

    # ä¿å­˜ç»“æœ
    import pickle
    from pathlib import Path

    # âœ… ç¡®ä¿ results/ ç›®å½•å­˜åœ¨
    ROOT = Path(__file__).resolve().parents[1]
    results_dir = ROOT / "results"
    results_dir.mkdir(exist_ok=True)  # âœ… åˆ›å»ºç›®å½•

    with open(results_dir / 'mstc_fl_history.pkl', 'wb') as f:
        pickle.dump(history, f)
    print(f"\nâœ… Results saved to {results_dir / 'mstc_fl_history.pkl'}")
