"""
è”é‚¦å­¦ä¹ å·¥ä½œæµç®¡ç†æ¨¡å—
"""
import os
os.environ['OMP_NUM_THREADS'] = '1'

import numpy as np
import torch
from typing import Dict, List, Tuple
from ..features.adaptive_mstr import AdaptiveMSTR
from ..features.multiscale_dtw import MultiScaleDTW
from ..features.adwin_scheduler_v2 import MultiClientADWIN
from ..clustering.hierarchical_gcn import HierarchicalGCNClustering


class WorkflowManager:
    """
    MSTC-FL å·¥ä½œæµç®¡ç†å™¨

    è´Ÿè´£ç®¡ç†æ•´ä¸ªè”é‚¦å­¦ä¹ çš„è¾…åŠ©æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - æ¼‚ç§»æ£€æµ‹ï¼ˆADWIN++ï¼‰
    - ç‰¹å¾æå–ï¼ˆMSTRï¼‰
    - è·ç¦»è®¡ç®—ï¼ˆMulti-Scale DTWï¼‰
    - ä¸‰å±‚å±‚æ¬¡èšç±»ï¼ˆMicro + Meso + Macroï¼‰
    - å®¢æˆ·ç«¯è®­ç»ƒç®¡ç†
    - é›†æˆæ¨¡å‹æ›´æ–°
    - å…¨å±€æ¨¡å‹è¯„ä¼°

    Args:
        n_clients: å®¢æˆ·ç«¯æ•°é‡
        use_timeseries_clustering: æ˜¯å¦ä½¿ç”¨æ—¶é—´åºåˆ—èšç±»æ¨¡å¼
        device: è®¡ç®—è®¾å¤‡

    Example:
        >>> workflow_mgr = WorkflowManager(n_clients=10, device=torch.device('cuda'))
        >>> workflow_mgr.set_components(
        ...     client_trainer=trainer,
        ...     model_aggregator=aggregator,
        ...     ensemble_manager=em_manager,
        ...     evaluator=evaluator
        ... )
    """

    def __init__(
            self,
            n_clients: int,
            use_timeseries_clustering: bool = True,
            device: torch.device = None
    ):
        self.n_clients = n_clients
        self.use_timeseries_clustering = use_timeseries_clustering
        self.device = device if device is not None else torch.device('cpu')

        # âœ… ä¾èµ–æ³¨å…¥å®¹å™¨ï¼ˆå»¶è¿Ÿè®¾ç½®ï¼‰
        self.client_trainer = None
        self.model_aggregator = None
        self.ensemble_manager = None
        self.evaluator = None

        # ========== ç«‹å³åˆå§‹åŒ–çš„æ ¸å¿ƒæ¨¡å— ==========

        # 1ï¸âƒ£ ADWIN++ æ¼‚ç§»æ£€æµ‹å™¨
        self.multi_client_adwin = MultiClientADWIN(
            n_clients=n_clients,
            delta=0.05,
            min_window=5,
            max_window=100,
            sensitivity=1.0
        )

        # 2ï¸âƒ£ Multi-Scale DTW è·ç¦»è®¡ç®—å™¨
        self.multiscale_dtw = MultiScaleDTW(
            scales=[5, 10, 20],
            weights=[0.5, 0.3, 0.2],
            mode='timeseries' if use_timeseries_clustering else 'features'
        )

        # 3ï¸âƒ£ å±‚æ¬¡ GCN èšç±»å™¨
        feature_dim = n_clients if use_timeseries_clustering else 16
        self.hierarchical_clustering = HierarchicalGCNClustering(
            n_clients=n_clients,
            feature_dim=feature_dim,
            embedding_dim=64,
            n_micro_clusters=5,
            n_meso_clusters=3,
            device=self.device
        )

        # 4ï¸âƒ£ MSTR ç‰¹å¾æå–å™¨ï¼ˆä»…ç‰¹å¾æ¨¡å¼éœ€è¦ï¼‰
        self.mstr_extractor = None
        if not use_timeseries_clustering:
            self.mstr_extractor = AdaptiveMSTR(
                base_scales={'micro': 5, 'meso': 15, 'macro': 40},
                delta=0.002,
                enable_adaptive=True
            )

    def set_components(
            self,
            client_trainer,
            model_aggregator,
            ensemble_manager,
            evaluator
    ):
        """
        è®¾ç½®ä¾èµ–ç»„ä»¶ï¼ˆä¾èµ–æ³¨å…¥ï¼‰

        Args:
            client_trainer: å®¢æˆ·ç«¯è®­ç»ƒå™¨
            model_aggregator: æ¨¡å‹èšåˆå™¨
            ensemble_manager: é›†æˆç®¡ç†å™¨
            evaluator: æ¨¡å‹è¯„ä¼°å™¨
        """
        self.client_trainer = client_trainer
        self.model_aggregator = model_aggregator
        self.ensemble_manager = ensemble_manager
        self.evaluator = evaluator

    def init_modules(self, feature_dim: int):
        """
        æ‰“å°åˆå§‹åŒ–ä¿¡æ¯

        Args:
            feature_dim: ç‰¹å¾ç»´åº¦ï¼ˆä»…ç”¨äºæ‰“å°ï¼‰
        """
        print(f"   âœ“ ADWIN++ detectors: {self.n_clients} instances")
        print(f"   âœ“ MultiScaleDTW: 3 scales")

        if not self.use_timeseries_clustering:
            print(f"   âœ“ MSTR feature extractor: {feature_dim} dims")
        else:
            print(f"   âœ“ MSTR feature extractor: Disabled (using timeseries)")

        print(f"   âœ“ Hierarchical GCN clustering")

    # ========================================
    # 1ï¸âƒ£ æ¼‚ç§»æ£€æµ‹
    # ========================================

    def detect_drift(self, client_accuracies: List[float]) -> Dict:
        """
        ADWIN++ æ¼‚ç§»æ£€æµ‹

        Args:
            client_accuracies: å®¢æˆ·ç«¯å‡†ç¡®ç‡åˆ—è¡¨ [N]

        Returns:
            æ¼‚ç§»ç»“æœå­—å…¸
            {
                'drift_clients': List[int],
                'n_drifts': int,
                'drift_dict': Dict[int, bool]
            }
        """
        # è°ƒç”¨ ADWIN++ æ£€æµ‹
        result = self.multi_client_adwin.update(client_accuracies)

        # æ„å»ºæ¼‚ç§»å­—å…¸
        drift_dict = {i: False for i in range(self.n_clients)}
        for client_id in result['drift_clients']:
            drift_dict[client_id] = True

        n_drifts = result['n_drifts']

        # æ‰“å°æ£€æµ‹ç»“æœ
        print(f"  Detected {n_drifts}/{self.n_clients} clients with drift")
        if n_drifts > 0:
            print(f"  Drifted clients: {result['drift_clients']}")

        return {
            'drift_clients': result['drift_clients'],
            'n_drifts': n_drifts,
            'drift_dict': drift_dict
        }

    # ========================================
    # 2ï¸âƒ£ DTW è·ç¦»è®¡ç®—
    # ========================================

    def compute_dtw_distances(
            self,
            client_timeseries: Dict[int, List[float]] = None,
            em_history: List[List[float]] = None
    ) -> np.ndarray:
        """
        è®¡ç®— DTW è·ç¦»çŸ©é˜µ

        Args:
            client_timeseries: å®¢æˆ·ç«¯æ—¶é—´åºåˆ— {client_id: [acc_1, acc_2, ...]}
            em_history: é›†æˆæˆå‘˜å†å² [n_rounds, n_ensemble]

        Returns:
            DTW è·ç¦»çŸ©é˜µ [n_clients, n_clients]
        """
        if self.use_timeseries_clustering:
            # æ—¶é—´åºåˆ—æ¨¡å¼ï¼šç›´æ¥è®¡ç®—æ—¶åº DTW
            client_timeseries_np = {
                i: np.array(ts) for i, ts in client_timeseries.items()
            }
            dtw_distances = self.multiscale_dtw.compute_distance_matrix_from_client_timeseries(
                client_timeseries_np
            )

            print(f"  DTW matrix: {dtw_distances.shape}")
            if dtw_distances[dtw_distances > 0].size > 0:
                print(f"  Distance range: [{dtw_distances[dtw_distances > 0].min():.4f}, "
                      f"{dtw_distances.max():.4f}]")

        else:
            # ç‰¹å¾æ¨¡å¼ï¼šå…ˆæå– MSTR ç‰¹å¾ï¼Œå†è®¡ç®— DTW
            mstr_features = self._extract_mstr_features(em_history)
            dtw_distances = self.multiscale_dtw.compute_distance_matrix(mstr_features)
            print(f"  DTW matrix: {dtw_distances.shape}")

        return dtw_distances

    def _extract_mstr_features(self, em_history: List[List[float]]) -> np.ndarray:
        """
        MSTR ç‰¹å¾æå–ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        Args:
            em_history: é›†æˆæˆå‘˜å†å² [n_rounds, n_ensemble]

        Returns:
            MSTR ç‰¹å¾çŸ©é˜µ [n_clients, feature_dim]
        """
        em_history_array = np.array(em_history)

        if len(em_history_array) == 0:
            print(f"  MSTR features: ({self.n_clients}, 16) (random initialization)")
            return np.random.randn(self.n_clients, 16)

        features_list = []
        n_ensemble = em_history_array.shape[1]

        for i in range(self.n_clients):
            em_idx = i % n_ensemble
            client_em_history = em_history_array[:, em_idx].reshape(-1, 1)

            if len(client_em_history) < 5:
                client_features = np.random.randn(16)
            else:
                current_variance = np.var(client_em_history[-5:])
                client_features = self.mstr_extractor.extract_features_adaptive(
                    client_em_history,
                    current_variance
                )

            features_list.append(client_features)

        features = np.array(features_list)
        print(f"  MSTR features: {features.shape}")

        return features

    # ========================================
    # 3ï¸âƒ£ ä¸‰å±‚å±‚æ¬¡èšç±»
    # ========================================

    def perform_clustering(
            self,
            dtw_distances: np.ndarray,
            round_idx: int = 0
    ) -> Dict:
        """
        æ‰§è¡Œä¸‰å±‚å±‚æ¬¡èšç±»

        Args:
            dtw_distances: DTW è·ç¦»çŸ©é˜µ [N, N]
            round_idx: å½“å‰è½®æ¬¡ï¼ˆç”¨äº Macro å±‚æ—¶åºåˆ†æï¼‰

        Returns:
            åŒ…å« micro, meso, macro ä¸‰å±‚ç»“æœçš„å­—å…¸
            {
                'round': int,
                'micro': {'labels': np.ndarray, 'n_clusters': int},
                'meso': {'labels': np.ndarray, 'n_clusters': int, 'embeddings': np.ndarray},
                'macro': {'labels': np.ndarray, 'n_clusters': int} or None
            }
        """
        # ä»è·ç¦»çŸ©é˜µæ„å»ºä¸´æ—¶ç‰¹å¾
        client_features = self.hierarchical_clustering._build_features_from_distance(
            dtw_distances
        )

        # è°ƒç”¨ä¸‰å±‚èšç±»æ–¹æ³•
        clustering_results = self.hierarchical_clustering.cluster_hierarchical(
            client_features=client_features,
            dtw_distances=dtw_distances,
            round_idx=round_idx
        )

        # æ‰“å°ä¸‰å±‚ç»“æœ
        print(f"\n  ğŸ“Š Three-Level Clustering (Round {round_idx}):")
        print(f"     Micro:  {clustering_results['micro']['n_clusters']} clusters")
        print(f"     Meso:   {clustering_results['meso']['n_clusters']} clusters")

        if clustering_results['macro'] is not None and 'n_clusters' in clustering_results['macro']:
            print(f"     Macro:  {clustering_results['macro']['n_clusters']} clusters")
        else:
            print(f"     Macro:  Not computed yet (need â‰¥5 rounds)")

        # æ‰“å° Micro ç°‡åˆ†å¸ƒ
        micro_labels = clustering_results['micro']['labels']
        for cluster_id in range(clustering_results['micro']['n_clusters']):
            members = np.where(micro_labels == cluster_id)[0]
            print(f"       Micro Cluster {cluster_id}: {len(members)} clients {list(members)}")

        return clustering_results

    # ========================================
    # 4ï¸âƒ£ å®¢æˆ·ç«¯è®­ç»ƒ
    # ========================================

    def train_clients(
            self,
            fed_data,
            ensemble: List,
            optimizers: List,
            schedulers: List,
            client_timeseries: Dict
    ) -> Tuple[List, List[float]]:
        """
        è®­ç»ƒæ‰€æœ‰å®¢æˆ·ç«¯

        Args:
            fed_data: è”é‚¦æ•°æ®å¯¹è±¡
            ensemble: é›†æˆæ¨¡å‹åˆ—è¡¨
            optimizers: ä¼˜åŒ–å™¨åˆ—è¡¨
            client_timeseries: å®¢æˆ·ç«¯æ—¶é—´åºåˆ—å†å²

        Returns:
            (client_models, client_accuracies)
        """
        # å§”æ‰˜ç»™ ClientTrainer
        client_results = self.client_trainer.train_clients(
            fed_data=fed_data,
            ensemble=ensemble,
            optimizers=optimizers,
            schedulers = schedulers
        )

        client_models = client_results['client_models']
        client_accuracies = client_results['client_accuracies']

        # æ›´æ–°æ—¶é—´åºåˆ—å†å²
        for client_id, acc in enumerate(client_accuracies):
            client_timeseries[client_id].append(acc)

        # æ‰“å°å®¢æˆ·ç«¯æ€§èƒ½
        print(f"  Client accuracies:")
        for i, acc in enumerate(client_accuracies):
            print(f"    Client {i}: {acc:.4f}")

        return client_models, client_accuracies

    # ========================================
    # 5ï¸âƒ£ é›†æˆæ¨¡å‹æ›´æ–°
    # ========================================

    def update_ensemble(
            self,
            cluster_representatives: Dict,
            client_accuracies: List[float],
            micro_clusters: Dict,
            ensemble: List
    ):
        """
        æ›´æ–°é›†æˆæ¨¡å‹

        Args:
            cluster_representatives: ç°‡ä»£è¡¨æ¨¡å‹ {cluster_id: model}
            client_accuracies: å®¢æˆ·ç«¯å‡†ç¡®ç‡ [N]
            micro_clusters: Micro ç°‡å­—å…¸ {cluster_id: [client_ids]}
            ensemble: é›†æˆæ¨¡å‹åˆ—è¡¨
        """
        # èšåˆç°‡ä»£è¡¨
        aggregated_models = list(cluster_representatives.values())

        print(f"  Updating ensemble from {len(aggregated_models)} cluster representatives...")

        # æ›´æ–°é›†æˆ
        self.ensemble_manager.update_ensemble(
            ensemble=ensemble,
            aggregated_models=aggregated_models,
            client_accuracies=client_accuracies
        )

        print(f"  Updated {len(ensemble)} ensemble members")

    # ========================================
    # 6ï¸âƒ£ å…¨å±€æ¨¡å‹è¯„ä¼°
    # ========================================

    # src/federated/workflow_manager.py
    # æ‰¾åˆ° global_evaluation() æ–¹æ³•ï¼ˆçº¦ Line 410-470ï¼‰ï¼Œå®Œå…¨æ›¿æ¢ä¸ºï¼š

    def global_evaluation(
            self,
            fed_data,
            ensemble: List,
            n_clients: int,
            global_history: List,
            round_idx: int,
            drift_results: Dict,
            clustering_results: Dict
    ) -> Dict:
        """
        å…¨å±€è¯„ä¼°ï¼ˆé›†æˆæŠ•ç¥¨ç‰ˆæœ¬ï¼‰

        Args:
            fed_data: è”é‚¦æ•°æ®å¯¹è±¡
            ensemble: é›†æˆæ¨¡å‹åˆ—è¡¨
            n_clients: å®¢æˆ·ç«¯æ•°é‡
            global_history: å…¨å±€å†å²è®°å½•
            round_idx: å½“å‰è½®æ¬¡
            drift_results: æ¼‚ç§»æ£€æµ‹ç»“æœ
            clustering_results: èšç±»ç»“æœ

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
            {
                'round': int,
                'accuracy': float,
                'f1_score': float,
                'precision': float,
                'recall': float,
                'n_drifts': int,
                'n_clusters': int
            }
        """
        # âœ… ä½¿ç”¨æ•´ä¸ªæµ‹è¯•é›†è¿›è¡Œè¯„ä¼°
        test_loader = fed_data.get_test_loader(batch_size=512)

        y_true_all = []
        y_pred_all = []

        # éå†æµ‹è¯•é›†æ‰¹æ¬¡
        for batch_X, batch_y in test_loader:
            batch_X = batch_X.to(self.device)
            batch_y = batch_y.to(self.device)

            # é›†æˆæŠ•ç¥¨
            predictions = []
            for model in ensemble:
                model.eval()
                with torch.no_grad():
                    logits = model(batch_X)
                    pred = torch.argmax(logits, dim=1)
                    predictions.append(pred)

            # å¤šæ•°æŠ•ç¥¨
            stacked_preds = torch.stack(predictions)  # (n_ensemble, batch_size)
            final_pred = torch.mode(stacked_preds, dim=0)[0]  # å¤šæ•°æŠ•ç¥¨

            y_true_all.append(batch_y.cpu().numpy())
            y_pred_all.append(final_pred.cpu().numpy())

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        y_true_all = np.concatenate(y_true_all)
        y_pred_all = np.concatenate(y_pred_all)

        # âœ… ä½¿ç”¨ç°æœ‰çš„ evaluator.evaluate_binary()
        metrics = self.evaluator.evaluate_binary(
            y_true=y_true_all,
            y_pred=y_pred_all
        )

        # âœ… è¡¥å……è½®æ¬¡ä¿¡æ¯
        metrics['round'] = round_idx
        metrics['n_drifts'] = drift_results.get('n_drifts', 0)

        # âœ… ä¿®å¤èšç±»ç»“æœæå–
        if 'micro' in clustering_results and 'labels' in clustering_results['micro']:
            metrics['n_clusters'] = len(np.unique(clustering_results['micro']['labels']))
        else:
            metrics['n_clusters'] = 0

        # è®°å½•å†å²
        global_history.append(metrics['f1_score'])

        # æ‰“å°è¯„ä¼°ç»“æœ
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  F1 Score: {metrics['f1_score']:.4f}")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall: {metrics['recall']:.4f}")

        return metrics
